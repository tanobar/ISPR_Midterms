{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49da7192",
   "metadata": {},
   "source": [
    "notes on sample_from_p()\n",
    "\n",
    "In a Restricted Boltzmann Machine (RBM), sampling is a key step in the training process, particularly during Gibbs sampling. The function sample_from_p is used to generate binary samples (0 or 1) from a given probability distribution p. These binary samples represent the activation states of the visible or hidden units in the RBM.\n",
    "\n",
    "How it works:\n",
    "Input: The function takes p, which is a tensor of probabilities (values between 0 and 1). Each value in p represents the probability of a unit being activated (set to 1).\n",
    "\n",
    "Random Sampling:\n",
    "\n",
    "torch.rand_like(p) generates a tensor of random values (uniformly distributed between 0 and 1) with the same shape as p.\n",
    "p - torch.rand_like(p) computes the difference between the probabilities and the random values.\n",
    "Thresholding:\n",
    "\n",
    "torch.sign(p - torch.rand_like(p)) produces a tensor where values greater than 0 are set to 1, and values less than or equal to 0 are set to -1. This effectively performs a thresholding operation to decide whether each unit is activated (1) or not (-1).\n",
    "ReLU Activation:\n",
    "\n",
    "F.relu(...) ensures that all negative values are clamped to 0. This step converts the -1 values to 0, resulting in a binary tensor of 0s and 1s."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a115011",
   "metadata": {},
   "source": [
    "notes on constrastive_divergence()\n",
    "\n",
    "Explanation:\n",
    "Positive Phase:\n",
    "\n",
    "Compute the hidden probabilities (p_h_given_v) and sample hidden states (h_sample).\n",
    "Compute the positive gradient as the outer product of h_sample and v.\n",
    "Negative Phase:\n",
    "\n",
    "Perform k steps of Gibbs sampling to reconstruct the visible and hidden states.\n",
    "Compute the negative gradient as the outer product of the reconstructed hidden probabilities and visible states.\n",
    "Weight and Bias Updates:\n",
    "\n",
    "Update the weights (W) and biases (v_bias, h_bias) using the difference between the positive and negative gradients, normalized by the batch size.\n",
    "Make sure to define a learning_rate attribute in your class (e.g., in the constructor) to control the step size for updates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca4154c",
   "metadata": {},
   "source": [
    "# Midterm 2, Assignment 3 - Gaetano Barresi [579102]\n",
    "\n",
    "A Restricted Boltzmann Machine (RBM) is a generative stochastic neural network that learns a probability distribution over its inputs. It is widely used for unsupervised learning, dimensionality reduction, and feature extraction.\n",
    "To implement from scratch a RBM we must define first its architecture. It is a simple two layer neural network, one input layer (visible states, our data) and one hidden layer (the hidden states, latent feature representation). For parameters, we have a set of weights and two set of bias:\n",
    "\n",
    "```python\n",
    "self.W = torch.randn(hidden_dim, visible_dim, device=self.device) * 0.01\n",
    "self.v_bias = torch.zeros(visible_dim, device=self.device)\n",
    "self.h_bias = torch.zeros(hidden_dim, device=self.device)\n",
    "```\n",
    "\n",
    "Weights W are initialized with small values and biases with zeros.\n",
    "\n",
    "Next step includes the implementation of sampling and probabilistic updates.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
