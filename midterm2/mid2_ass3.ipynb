{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3549a26",
   "metadata": {},
   "source": [
    "The gradients computed in the `contrastive_divergence()` function using `torch.mm()` are **outer products**, and this is a key concept in training Restricted Boltzmann Machines (RBMs). Let me break it down step by step:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **What is `torch.mm()` Doing?**\n",
    "`torch.mm(A, B)` performs a **matrix multiplication** between two tensors `A` and `B`. In this case:\n",
    "- `positive_grad = torch.mm(h_sample.t(), v)` computes the outer product of the hidden activations (`h_sample`) and the visible units (`v`) during the **positive phase**.\n",
    "- `negative_grad = torch.mm(p_h_given_v_sample.t(), v_sample)` computes the outer product of the hidden activations (`p_h_given_v_sample`) and the reconstructed visible units (`v_sample`) during the **negative phase**.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Why Outer Products?**\n",
    "The **outer product** is used because the weight matrix `W` in an RBM connects every visible unit to every hidden unit. The weight update rule in RBMs is based on the correlations between visible and hidden units. Specifically:\n",
    "- The **positive phase** captures the correlation between the visible units (`v`) and the hidden units (`h_sample`) when the model is exposed to the real data.\n",
    "- The **negative phase** captures the correlation between the reconstructed visible units (`v_sample`) and the hidden units (`p_h_given_v_sample`) after Gibbs sampling.\n",
    "\n",
    "The outer product is the natural way to compute these correlations because:\n",
    "- The outer product of a column vector (e.g., `h_sample`) and a row vector (e.g., `v`) results in a matrix where each element represents the interaction (or correlation) between a specific hidden unit and a specific visible unit.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Positive and Negative Gradients**\n",
    "- **Positive Gradient (`positive_grad`)**:\n",
    "  - This is computed using the real data (`v`) and the hidden activations (`h_sample`).\n",
    "  - It represents how the weights should be adjusted to increase the probability of the observed data.\n",
    "  - It is called \"positive\" because it reinforces the correlations between visible and hidden units for the real data.\n",
    "\n",
    "- **Negative Gradient (`negative_grad`)**:\n",
    "  - This is computed using the reconstructed data (`v_sample`) and the hidden activations (`p_h_given_v_sample`).\n",
    "  - It represents how the weights should be adjusted to decrease the probability of the reconstructed data (which the model generates itself).\n",
    "  - It is called \"negative\" because it counteracts the model's tendency to overfit to its own reconstructions.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Why Does This Work?**\n",
    "The goal of training an RBM is to adjust the weights (`W`) such that the model assigns higher probabilities to the real data and lower probabilities to the reconstructed data. This is achieved by:\n",
    "- **Maximizing the log-likelihood** of the data, which involves computing the gradient of the log-likelihood with respect to the weights.\n",
    "- The gradient of the log-likelihood can be approximated using **Contrastive Divergence (CD)**, which uses the difference between the positive and negative gradients:\n",
    "  ```python\n",
    "  Î”W = learning_rate * (positive_grad - negative_grad) / batch_size\n",
    "  ```\n",
    "  This update rule ensures that the weights are adjusted to better represent the real data while discouraging the model from overfitting to its own reconstructions.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Why Outer Products Specifically?**\n",
    "The weight matrix `W` in an RBM connects every visible unit to every hidden unit. The outer product is used because:\n",
    "- Each weight `W[i, j]` represents the connection between visible unit `i` and hidden unit `j`.\n",
    "- The outer product computes all these pairwise interactions (correlations) at once, resulting in a matrix of the same shape as `W`.\n",
    "\n",
    "For example:\n",
    "- If `v` has shape `(batch_size, visible_dim)` and `h_sample` has shape `(batch_size, hidden_dim)`, then:\n",
    "  - `h_sample.t()` has shape `(hidden_dim, batch_size)`.\n",
    "  - `torch.mm(h_sample.t(), v)` results in a matrix of shape `(hidden_dim, visible_dim)`, which matches the shape of `W`.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Summary**\n",
    "- The **positive gradient** captures correlations between real data and hidden units, reinforcing the model's ability to represent the data.\n",
    "- The **negative gradient** captures correlations between reconstructed data and hidden units, discouraging overfitting to the model's own reconstructions.\n",
    "- Both gradients are computed as **outer products** because the weight matrix `W` connects every visible unit to every hidden unit, and the outer product naturally captures these pairwise interactions.\n",
    "- The weight update rule (`W += learning_rate * (positive_grad - negative_grad)`) ensures the model learns to better represent the real data while avoiding overfitting.\n",
    "\n",
    "This is why the outer product and the distinction between positive and negative gradients are central to training RBMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca4154c",
   "metadata": {},
   "source": [
    "# Midterm 2, Assignment 3 - Gaetano Barresi [579102]\n",
    "\n",
    "A Restricted Boltzmann Machine (RBM) is a generative stochastic neural network that learns a probability distribution over its inputs. It is widely used for unsupervised learning, dimensionality reduction, and feature extraction.\n",
    "To implement from scratch a RBM we must define first its architecture. It is a simple two layer neural network, one input layer (visible states, our data) and one hidden layer (the hidden states, latent feature representation). For parameters, we have a set of weights and two set of bias, one for the visible units and one for the hidden units:\n",
    "\n",
    "\n",
    "```python\n",
    "self.W = torch.randn(hidden_dim, visible_dim, device=self.device) * 0.01\n",
    "self.v_bias = torch.zeros(visible_dim, device=self.device)\n",
    "self.h_bias = torch.zeros(hidden_dim, device=self.device)\n",
    "```\n",
    "\n",
    "\n",
    "Weights W are initialized with small values and biases with zeros.\n",
    "Hidden units are conditionally independent given visible units and viceversa, due to not oriented edges and bipartition structure:\n",
    "\n",
    "\n",
    "$$\n",
    "\\\n",
    "\\mathbb{P}(h_j \\mid v) = \\sigma \\left( \\sum_i M_{ij} v_i + c_j \\right) \\quad \\forall j\n",
    "\\\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\\n",
    "\\mathbb{P}(v_i \\mid h) = \\sigma \\left( \\sum_j M_{ij} h_j + b_i \\right) \\quad \\forall i\n",
    "\\\n",
    "$$\n",
    "\n",
    "\n",
    "These are resepctively the forward pass (wake) and the backward pass (dream) and they can be implemented as:\n",
    "\n",
    "\n",
    "```python\n",
    "def visible_to_hidden(self, v):  # forward pass\n",
    "    # compute probabilities of hidden units given visible units\n",
    "    p_h = torch.sigmoid(F.linear(v, self.W, self.h_bias))\n",
    "    return p_h\n",
    "    \n",
    "def hidden_to_visible(self, h):  # backward pass\n",
    "    # compute probabilities of visible units given hidden units\n",
    "    p_v = torch.sigmoid(F.linear(h, self.W.t(), self.v_bias))\n",
    "    return p_v\n",
    "```\n",
    "\n",
    "\n",
    "In a RBM, sampling is a key step in the training process, particularly during Gibbs sampling. In order to generate binary samples (0 or 1) from a given probability distribution p, we use the following function. These binary samples represent the activation states of the visible or hidden units in the RBM.\n",
    "\n",
    "\n",
    "```python\n",
    "def sample_from_p(self, p):\n",
    "    # bernoulli sampling given probabilities\n",
    "    return F.relu(torch.sign(p - torch.rand_like(p, device=self.device)))\n",
    "```\n",
    "\n",
    "\n",
    "`sample_from_p` takes p, which is a tensor of probabilities. Each value in p represents the probability of a unit being activated (set to 1).\n",
    "`torch.rand_like(p)` generates a tensor of random values, uniformly distributed between 0 and 1, with the same shape as p. `p - torch.rand_like(p)` computes the difference between the probabilities and the random values.\n",
    "`torch.sign(p - torch.rand_like(p))` produces a tensor where values greater than 0 are set to 1, and values less than or equal to 0 are set to -1. This effectively performs a thresholding operation to decide whether each unit is activated (1) or not (-1).\n",
    "`F.relu(...)` ensures that all negative values are clamped to 0. This step converts the -1 values to 0, resulting in a binary tensor of 0s and 1s.\n",
    "All these pieces are used inside the generalized version of Contrastive Divergence (CD) learning algorithm. It is divided in positive phase (wake part), negative phase(dream part) and parameters update.\n",
    "Positive phase computes the hidden probabilities (`p_h_given_v`) and sample hidden states (`h_sample`). It computes also the positive gradient as the outer product of `h_sample` and `v`.\n",
    "\n",
    "\n",
    "```python\n",
    "# positive phase\n",
    "p_h_given_v = self.visible_to_hidden(v)\n",
    "h_sample = self.sample_from_p(p_h_given_v)\n",
    "positive_grad = torch.mm(h_sample.t(), v)\n",
    "```\n",
    "\n",
    "\n",
    "Negative phase performs k steps of Gibbs sampling to reconstruct the visible and hidden states and computes the negative gradient as the outer product of the reconstructed hidden probabilities and visible states.\n",
    "\n",
    "\n",
    "```python\n",
    "# gibbs sampling (negative phase)\n",
    "v_sample = v\n",
    "for _ in range(self.k):\n",
    "    p_h_given_v = self.visible_to_hidden(v_sample)\n",
    "    h_sample = self.sample_from_p(p_h_given_v)\n",
    "    p_v_given_h = self.hidden_to_visible(h_sample)\n",
    "    v_sample = self.sample_from_p(p_v_given_h)\n",
    "\n",
    "# negative phase\n",
    "p_h_given_v_sample = self.visible_to_hidden(v_sample)\n",
    "negative_grad = torch.mm(p_h_given_v_sample.t(), v_sample)\n",
    "```\n",
    "\n",
    "\n",
    "Weights (`W`) and biases (`v_bias`, `h_bias`) are updated using the difference between the positive and negative gradients, normalized by the batch size.\n",
    "\n",
    "\n",
    "```python\n",
    "self.W += self.learning_rate * (positive_grad - negative_grad) / v.size(0)\n",
    "self.v_bias += self.learning_rate * torch.sum(v - v_sample, dim=0) / v.size(0)\n",
    "self.h_bias += self.learning_rate * torch.sum(p_h_given_v - p_h_given_v_sample, dim=0) / v.size(0)\n",
    "```\n",
    "\n",
    "\n",
    "We now pack all this stuff in a custom class `RBM`, provided with its own training method and train two different RBMs, with different values of k for CD algorithm: k=4 and k=8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9229443",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "class RBM:\n",
    "    def __init__(self, visible_dim, hidden_dim, k=1, lr=0.01, device='cpu'):\n",
    "        self.visible_dim = visible_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.k = k  # number of Gibbs sampling steps\n",
    "        self.learning_rate = lr\n",
    "        self.device = device  # Device to run the computations on (e.g., 'cuda' or 'cpu')\n",
    "        \n",
    "        # weights and biases initialization\n",
    "        self.W = torch.randn(hidden_dim, visible_dim, device=self.device) * 0.01\n",
    "        self.v_bias = torch.zeros(visible_dim, device=self.device)\n",
    "        self.h_bias = torch.zeros(hidden_dim, device=self.device)\n",
    "    \n",
    "    def sample_from_p(self, p):\n",
    "        # bernoulli sampling given probabilities\n",
    "        return F.relu(torch.sign(p - torch.rand_like(p, device=self.device)))\n",
    "    \n",
    "    def visible_to_hidden(self, v):  # forward pass\n",
    "        # compute probabilities of hidden units given visible units\n",
    "        p_h = torch.sigmoid(F.linear(v, self.W, self.h_bias))\n",
    "        return p_h\n",
    "    \n",
    "    def hidden_to_visible(self, h):  # backward pass\n",
    "        # compute probabilities of visible units given hidden units\n",
    "        p_v = torch.sigmoid(F.linear(h, self.W.t(), self.v_bias))\n",
    "        return p_v\n",
    "\n",
    "    def contrastive_divergence(self, v):\n",
    "        # Move input to the correct device\n",
    "        v = v.to(self.device)\n",
    "\n",
    "        # positive phase\n",
    "        p_h_given_v = self.visible_to_hidden(v)\n",
    "        h_sample = self.sample_from_p(p_h_given_v)\n",
    "        positive_grad = torch.mm(h_sample.t(), v)\n",
    "\n",
    "        # gibbs sampling (negative phase)\n",
    "        v_sample = v\n",
    "        for _ in range(self.k):\n",
    "            p_h_given_v = self.visible_to_hidden(v_sample)\n",
    "            h_sample = self.sample_from_p(p_h_given_v)\n",
    "            p_v_given_h = self.hidden_to_visible(h_sample)\n",
    "            v_sample = self.sample_from_p(p_v_given_h)\n",
    "\n",
    "        # negative phase\n",
    "        p_h_given_v_sample = self.visible_to_hidden(v_sample)\n",
    "        negative_grad = torch.mm(p_h_given_v_sample.t(), v_sample)\n",
    "\n",
    "        # update weights and biases\n",
    "        self.W += self.learning_rate * (positive_grad - negative_grad) / v.size(0)\n",
    "        self.v_bias += self.learning_rate * torch.sum(v - v_sample, dim=0) / v.size(0)\n",
    "        self.h_bias += self.learning_rate * torch.sum(p_h_given_v - p_h_given_v_sample, dim=0) / v.size(0)\n",
    "\n",
    "    def train(self, data_loader, epochs=10):\n",
    "        for epoch in range(epochs):\n",
    "            epoch_error = 0\n",
    "            for batch in tqdm(data_loader, desc=\"Training Batches\", leave=False):\n",
    "                # Extract data from batch (ignore labels)\n",
    "                batch, _ = batch  # Unpack the tuple (data, labels)\n",
    "                batch = batch.view(-1, self.visible_dim).to(self.device)  # Flatten input and move to device\n",
    "                self.contrastive_divergence(batch)\n",
    "                \n",
    "                # Compute reconstruction error\n",
    "                v_reconstructed = self.hidden_to_visible(self.sample_from_p(self.visible_to_hidden(batch)))\n",
    "                epoch_error += torch.sum((batch - v_reconstructed) ** 2).item()\n",
    "            \n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Reconstruction Error: {epoch_error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb60c263",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "visible_dim = 784  # For MNIST\n",
    "hidden_dim = 256  # Number of hidden neurons in RBM\n",
    "num_classes = 10  # Digits 0-9\n",
    "\n",
    "# Use GPU if available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize RBMs\n",
    "rbm4 = RBM(visible_dim, hidden_dim, k=4, lr=0.01, device=device)\n",
    "rbm8 = RBM(visible_dim, hidden_dim, k=8, lr=0.01, device=device)\n",
    "\n",
    "# Load the MNIST training and test data\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: x.view(-1))])\n",
    "mnist_train_data = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(mnist_train_data, batch_size=10, shuffle=True)\n",
    "\n",
    "print(\"Training RBM4...\")\n",
    "rbm4.train(train_loader, epochs=100)\n",
    "print(\"Training RBM8...\")\n",
    "rbm8.train(train_loader, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91a4a3c",
   "metadata": {},
   "source": [
    "Once we have the trained RBMs we need to create an encoding of MNIST dataset using their hidden neurons. We encode the MNIST twice, one time for each RBM and we will use these encodings to train a couple of simple classifiers. Their performances will reflect the RBMs' encodings quality.\n",
    "\n",
    "The following code block will show the implementation of the encoding function and its call. The code block after will show the implementation of the classifier and its training loop, with its training phase and evaluation phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87ce4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_dataset(rbm, data_loader, device):\n",
    "    # Lists to store encodings and labels\n",
    "    encodings = []\n",
    "    labels = []\n",
    "\n",
    "    # Iterate through the dataset\n",
    "    for batch, batch_labels in data_loader:\n",
    "        batch = batch.to(device)  # Move to the correct device\n",
    "        # Compute hidden neuron activations for the entire batch\n",
    "        hidden_activations = rbm.visible_to_hidden(batch)\n",
    "        encodings.append(hidden_activations.cpu())  # Store encodings\n",
    "        labels.append(batch_labels)  # Store labels\n",
    "\n",
    "    # Concatenate all batches into a single tensor\n",
    "    encodings = torch.cat(encodings, dim=0)\n",
    "    labels = torch.cat(labels, dim=0)\n",
    "\n",
    "    return encodings, labels\n",
    "\n",
    "############################################################\n",
    "\n",
    "mnist_test_data = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(mnist_test_data, batch_size=10, shuffle=False)\n",
    "\n",
    "rbm4_train_encodings, rbm4_train_labels = encode_dataset(rbm4, train_loader, device)\n",
    "rbm4_test_encodings, rbm4_test_labels = encode_dataset(rbm4, test_loader, device)\n",
    "\n",
    "rbm8_train_encodings, rbm8_train_labels = encode_dataset(rbm8, train_loader, device)\n",
    "rbm8_test_encodings, rbm8_test_labels = encode_dataset(rbm8, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baef572a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class MNISTClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(MNISTClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "    \n",
    "\n",
    "def train_classifier_on_mnist(train_loader, input_dim, num_classes, device, num_epochs=40, lr=0.001):\n",
    "    # Define the classifier\n",
    "    classifier = MNISTClassifier(input_dim, num_classes).to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(classifier.parameters(), lr=lr)\n",
    "\n",
    "    # Training loop\n",
    "    print(\"Training classifier...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        classifier.train()\n",
    "        epoch_loss = 0\n",
    "        for batch, labels in train_loader:\n",
    "            batch, labels = batch.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = classifier(batch)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    return classifier\n",
    "\n",
    "\n",
    "def evaluate_classifier(classifier, test_loader, device):\n",
    "    classifier.eval()  # Set the classifier to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "        for batch, labels in test_loader:\n",
    "            batch, labels = batch.to(device), labels.to(device)\n",
    "            outputs = classifier(batch)\n",
    "            _, predicted = torch.max(outputs, 1)  # Get the class with the highest score\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "#############################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
