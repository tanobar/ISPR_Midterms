{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffcc2ead",
   "metadata": {},
   "source": [
    "# Midterm 3, Assignment 1 - Gaetano Barresi [579102]\n",
    "\n",
    "A Denoising Autoencoder (DAE) is a neural network that learns robust latent representations by reconstructing clean inputs from corrupted data. Unlike standard autoencoders that simply reconstruct the original inputs, a DAE must remove noise from artificially corrupted inputs and discover stable features that capture the true data distribution. The goal of this work is to train two versions of a DAE on the CIFAR10 dataset, one using dense layers and one using convolutional layers and to show an accuracy comparison between them.\n",
    "\n",
    "The architecture of a DAE has the following form:\n",
    "\n",
    "$$\n",
    "\\\n",
    "[Input] ‚Üí [Corruption] ‚Üí [Encoder] ‚Üí [Latent Code] ‚Üí [Decoder] ‚Üí [Reconstruction]\n",
    "\\\n",
    "$$\n",
    "\n",
    "$[Input]$ is self-explanatory. $[Corruption]$ is the stage where we inject artificial noise to the input through some noise process $C(\\hat{x} ‚à£ x)$ and obtain $\\hat{x}$, a corrupted version of the original input. In this work the corruption process is done via standard Gaussian noise, so:\n",
    "\n",
    "$$\n",
    "\\\n",
    "\\hat{x} = x + œµ, œµ ‚àº ùí©(0, œÉ^2).\n",
    "\\\n",
    "$$\n",
    "\n",
    "$[Encoder]$ $f()$ maps noisy input $\\hat{x}$ to latent representation $z$:\n",
    "\n",
    "$$\n",
    "\\\n",
    "z = f(\\hat{x}) ‚àà ‚Ñù·µà,\n",
    "\\\n",
    "$$\n",
    "\n",
    "where $d << input\\_dim$. Learned $[Latent Code]$ $z$ should be robust to partial destruction of the input.\n",
    "$[Decoder]$ $g()$ reconstructs clean input from $z$:\n",
    "\n",
    "$$\n",
    "\\\n",
    "x' = g(z) ‚âà x,\n",
    "\\\n",
    "$$\n",
    "\n",
    "where $x'$ is our $[Reconstruction]$.\n",
    "\n",
    "The \"magic\" of DAEs lies in their bottleneck architecture. As data passes through the encoder, the network gradually compresses the input while stripping away noise and preserving only the most essential features. This forced dimensionality reduction creates a distilled latent representation containing only the core patterns needed for reconstruction. The decoder then reverses this process, carefully rebuilding the clean input from these compressed features layer by layer. By training on noisy-clean pairs, the network learns to discard random corruptions during compression while maintaining the structural integrity needed for accurate reconstruction.\n",
    "\n",
    "More formally, the DAE is trained to minimize the loss function\n",
    "\n",
    "$$\n",
    "\\\n",
    "L = (x, g(f(\\hat{x}))),\n",
    "\\\n",
    "$$\n",
    "\n",
    "or, using a probabilistic interpretation, the DAE learns the denoising distribuition\n",
    "\n",
    "$$\n",
    "\\\n",
    "P(x|\\hat{x})\n",
    "\\\n",
    "$$\n",
    "\n",
    "by minimizing\n",
    "\n",
    "$$\n",
    "\\\n",
    "-log P(x|z = f(\\hat{x})).\n",
    "\\\n",
    "$$\n",
    "\n",
    "After this brief introduction, we proceed with the implementation. First, we need to load and preprocess the dataset. We normalize the CIFAR-10 data to the range [-1, 1] to achieve several benefits such as: compatibility with the tanh activation, effective noise handling due to the symmetric range that ensures Gaussian noise is equally well-handled in both positive and negative directions, gradient flow optimization, numerical stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac25293e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# transform for CIFAR-10, normalizing to [-1, 1]\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Range: [-1, 1]\n",
    "])\n",
    "# Load TR set\n",
    "train_data = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_data, batch_size=100, shuffle=True)\n",
    "# Load TS set\n",
    "test_data = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(test_data, batch_size=100, shuffle=False)\n",
    "\n",
    "print(\"Data loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084f0f1b",
   "metadata": {},
   "source": [
    "To implement the DAE, we subclass PyTorch‚Äôs `nn.Module` base class, inheriting its core functionality for neural network operations. With a single class we can specify which kind of DAE to instantiate, a dense or a convolutional one.\n",
    "\n",
    "In convolutional DAE's architecture, the encoder part processes input images through successive 3x3 convolutional kernels: first expanding to 64 channels while maintaining the original 32x32 resolution via padding, then downsampling to 16x16 through max pooling, followed by 128-channel convolution and another downsampling to 8x8, before final compression into 256 channels. Each convolutional operation is followed by ReLU activation for non-linearity. The decoder mirrors this architecture with 3x3 convolutions, first expanding from 256 to 128 channels at 8x8 resolution, then upsampling to 16x16, followed by 64-channel convolution and final upsampling back to the original 32x32 dimensions. The reconstruction concludes with a 3-channel output using tanh activation to match the normalized input range.\n",
    "\n",
    "The dense DAE employs a symmetrical fully-connected architecture with three encoding layers and three decoding layers. The encoder progressively compresses the input through linear transformations: first reducing the 3072-dimensional input to 1024 neurons, then to 512 neurons, and finally to a 256 neuron bottleneck layer. Each layer uses ReLU activation to introduce non-linearity while maintaining sparse representations. The decoder precisely mirrors this structure in reverse order, expanding from 256 to 512 neurons, then to 1024 neurons, before reconstructing the original 3072 dimensional output. The final layer applies tanh activation, chosen specifically for its compatibility with our [-1, 1] normalized inputs. This symmetric compression-decompression pathway, combined with tanh's balanced gradient properties and zero-centered output range, enables effective reconstruction while avoiding the positive-value bias that would occur with sigmoid activation.\n",
    "\n",
    "The class has methods to train and evaluate the DAE:\n",
    "- `train_model()` is a classical training loop. We use MSE loss, Adam optimizer and a learnig rate of 1e-4. We also use the function `add_noise()` inside the training loop\n",
    "to corrupt the images with a noise factor of $\\sigma = 0.2$, a standard value with a good robustness/quality trade-off.\n",
    "\n",
    "- `evaluate()` is the function used to quantitatively assess our DAE's reconstruction quality. We employ two metrics: Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM).\n",
    "\n",
    "PSNR measures pixel-level accuracy between clean and reconstructed images. It directly relates to the MSE loss used during training\n",
    "\n",
    "$$\n",
    "\\\n",
    "PSNR = 10 \\cdot \\log_{10}\\left(\\frac{MAX_I^2}{MSE}\\right),\n",
    "\\\n",
    "$$\n",
    "\n",
    "where $MAX_I$ is the maximum possible pixel value and $MSE$ is the mean squared error across all pixels/channels.\n",
    "Higher values indicates better recontruction. In CIFAR-10 a typical range is 25-35 dB.\n",
    "PSNR uses the decibel (dB) scale because it quantifies the ratio between maximum possible signal power and noise power, so the image's pixel value range and reconstruction error.\n",
    "\n",
    "SSIM evaluates perceptual quality by comparing luminance (mean intensity), contrast (standard deviation) and structure (normalized cross-correlation). It is a metric that matches human vision better than pixel-wise metrics.\n",
    "\n",
    "$$\n",
    "\\\n",
    "SSIM(x,y) = \\frac{(2\\mu_x\\mu_y + C_1)(2\\sigma_{xy} + C_2)}{(\\mu_x^2 + \\mu_y^2 + C_1)(\\sigma_x^2 + \\sigma_y^2 + C_2)},\n",
    "\\\n",
    "$$\n",
    "\n",
    "where $\\mu$ is local mean, $\\sigma$ is standard deviation, $\\sigma_{xy}$ is cross-covariance and $C_1, C_2$ are stability costants.\n",
    "SSIM ranges [-1, 1] with 1 = perfect match. Values > 0.8 indicate good recontruction; values > 0.9 indicate excellent reconstruction.\n",
    "\n",
    "- `visualize_result()` is just a function to plot noisy vs reconstructed vs clean images and have a visualization of DAE's performance.\n",
    "\n",
    "Inside the next code block we can find the implementation of the whole class. The training and test will follow immediately afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebefd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class DAE(nn.Module):\n",
    "    def __init__(self, mode=None):\n",
    "        super(DAE, self).__init__()\n",
    "\n",
    "        self.mode = mode\n",
    "\n",
    "        if mode == 'conv':\n",
    "            # Convolutional Encoder\n",
    "            self.encoder = nn.Sequential(\n",
    "                nn.Conv2d(3, 64, 3, padding=1),   # 32x32\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Conv2d(64, 128, 3, padding=1), # 16x16\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Conv2d(128, 256, 3, padding=1) # 8x8\n",
    "            )\n",
    "            # Convolutional Decoder\n",
    "            self.decoder = nn.Sequential(\n",
    "                nn.Conv2d(256, 128, 3, padding=1),  # 8x8\n",
    "                nn.ReLU(),\n",
    "                nn.Upsample(scale_factor=2),\n",
    "                nn.Conv2d(128, 64, 3, padding=1),   # 16x16\n",
    "                nn.ReLU(),\n",
    "                nn.Upsample(scale_factor=2),\n",
    "                nn.Conv2d(64, 3, 3, padding=1),     # 32x32\n",
    "                nn.Tanh()\n",
    "            )\n",
    "        elif mode == 'dense':\n",
    "            # Dense Encoder\n",
    "            self.encoder = nn.Sequential(\n",
    "                nn.Linear(3 * 32 * 32, 1024),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(1024, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(512, 256),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "            # Dense Decoder\n",
    "            self.decoder = nn.Sequential(\n",
    "                nn.Linear(256, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(512, 1024),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(1024, 3 * 32 * 32),\n",
    "                nn.Tanh()\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"Invalid mode. Choose 'conv' or 'dense'.\")\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.mode == 'conv':\n",
    "            x = self.encoder(x)\n",
    "            x = self.decoder(x)\n",
    "        else:\n",
    "            x = x.view(x.size(0), -1)  # Flatten to (batch_size, 3072)\n",
    "            x = self.encoder(x)\n",
    "            x = self.decoder(x)\n",
    "            x = x.view(-1, 3, 32, 32)  # Reshape to image dimensions\n",
    "        return x\n",
    "    \n",
    "\n",
    "    def add_noise(self, inputs, noise_factor=0.2):\n",
    "        noise = noise_factor * torch.randn_like(inputs)\n",
    "        noisy = inputs + noise\n",
    "        return torch.clamp(noisy, -1., 1.)  # CIFAR-10 is normalized to [-1, 1]\n",
    "\n",
    "\n",
    "    def train_model(self, train_loader, num_epochs=20, lr=1e-4, print_interval=100):\n",
    "        #parameter print_interval (int): Print loss every N examples\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.to(device)\n",
    "        \n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        \n",
    "        self.train()  # Set to training mode\n",
    "        for epoch in range(num_epochs):\n",
    "            for batch_idx, (clean_imgs, _) in enumerate(train_loader):\n",
    "                clean_imgs = clean_imgs.to(device)\n",
    "                \n",
    "                # Add noise and reconstruct\n",
    "                noisy_imgs = self.add_noise(clean_imgs)\n",
    "                reconstructed = self(noisy_imgs)\n",
    "                \n",
    "                # Compute loss and update\n",
    "                loss = self.criterion(reconstructed, clean_imgs)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                # Print progress\n",
    "                if batch_idx % print_interval == 0:\n",
    "                    print(f\"Epoch [{epoch+1}/{num_epochs}], Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "\n",
    "    def evaluate(self, test_loader, noise_factor=0.2):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.to(device)\n",
    "\n",
    "        #Compute PSNR and SSIM on test set\n",
    "        self.eval()  # Set to evaluation mode\n",
    "        total_psnr = 0.0\n",
    "        total_ssim = 0.0\n",
    "        num_samples = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for clean_imgs, _ in test_loader:\n",
    "                clean_imgs = clean_imgs.to(device)\n",
    "                noisy_imgs = self.add_noise(clean_imgs, noise_factor)\n",
    "                reconstructed = self(noisy_imgs)\n",
    "                \n",
    "                # Convert to numpy (CPU) for metric calculation\n",
    "                clean_np = clean_imgs.cpu().numpy()\n",
    "                recon_np = reconstructed.cpu().numpy()\n",
    "                \n",
    "                # Compute metrics per image\n",
    "                for i in range(clean_np.shape[0]):\n",
    "                    # PSNR (higher is better)\n",
    "                    total_psnr += psnr(clean_np[i], recon_np[i], data_range=2.0)  # data_range=2 for [-1,1]\n",
    "                    \n",
    "                    # SSIM (higher is better, multichannel=True for RGB)\n",
    "                    total_ssim += ssim(clean_np[i].transpose(1,2,0), \n",
    "                                     recon_np[i].transpose(1,2,0), \n",
    "                                     data_range=2.0, \n",
    "                                     channel_axis=2)\n",
    "                \n",
    "                num_samples += clean_np.shape[0]\n",
    "        \n",
    "        avg_psnr = total_psnr / num_samples\n",
    "        avg_ssim = total_ssim / num_samples\n",
    "        return avg_psnr, avg_ssim\n",
    "    \n",
    "\n",
    "    def visualize_results(self, test_loader, num_images=5):\n",
    "        #Plot noisy vs reconstructed vs clean images\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.to(device)\n",
    "\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            clean_imgs, _ = next(iter(test_loader))\n",
    "            clean_imgs = clean_imgs.to(device)[:num_images]\n",
    "            noisy_imgs = self.add_noise(clean_imgs)\n",
    "            reconstructed = self(noisy_imgs)\n",
    "            \n",
    "            # Denormalize to [0,1] for plotting\n",
    "            clean_imgs = (clean_imgs + 1) / 2\n",
    "            noisy_imgs = (noisy_imgs + 1) / 2\n",
    "            reconstructed = (reconstructed + 1) / 2\n",
    "            \n",
    "            fig, axes = plt.subplots(num_images, 3, figsize=(10, num_images*2))\n",
    "            for i in range(num_images):\n",
    "                axes[i,0].imshow(noisy_imgs[i].cpu().permute(1,2,0))\n",
    "                axes[i,0].set_title(\"Noisy\")\n",
    "                axes[i,1].imshow(reconstructed[i].cpu().permute(1,2,0))\n",
    "                axes[i,1].set_title(\"Reconstructed\")\n",
    "                axes[i,2].imshow(clean_imgs[i].cpu().permute(1,2,0))\n",
    "                axes[i,2].set_title(\"Clean\")\n",
    "                for ax in axes[i]:\n",
    "                    ax.axis('off')\n",
    "            plt.tight_layout()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c11967f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Convolutional DAE model\n",
    "cdae = DAE(mode='conv')\n",
    "cdae.train_model(train_loader)\n",
    "print(\"CDAE model ready.\")\n",
    "\n",
    "# Initialize Dense DAE model\n",
    "ddae = DAE(mode='dense')\n",
    "ddae.train_model(train_loader)\n",
    "print(\"DDAE model ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55021519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantitative evaluation CDAE\n",
    "psnr_score, ssim_score = cdae.evaluate(test_loader)\n",
    "print(f\"Test PSNR: {psnr_score:.2f} dB, SSIM: {ssim_score:.4f}\")\n",
    "# Qualitative visualization CDAE\n",
    "cdae.visualize_results(test_loader)\n",
    "\n",
    "# Quantitative evaluation DDAE\n",
    "psnr_score, ssim_score = ddae.evaluate(test_loader)\n",
    "print(f\"Test PSNR: {psnr_score:.2f} dB, SSIM: {ssim_score:.4f}\")\n",
    "# Qualitative visualization DDAE\n",
    "ddae.visualize_results(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301a3984",
   "metadata": {},
   "source": [
    "Conclusions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mid3_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
