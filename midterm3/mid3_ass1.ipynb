{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6d7a53e",
   "metadata": {},
   "source": [
    "NOTES\n",
    "\n",
    "SSIM loss(?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcc2ead",
   "metadata": {},
   "source": [
    "# Midterm 3, Assignment 1 - Gaetano Barresi [579102]\n",
    "\n",
    "A Denoising Autoencoder (DAE) is a neural network that learns robust latent representations by reconstructing clean inputs from corrupted data. Unlike standard autoencoders that simply reconstruct the original inputs, a DAE must remove noise from artificially corrupted inputs and discover stable features that capture the true data distribution. The goal of this work is to train two versions of a DAE on the CIFAR10 dataset, one using dense layers and one using convolutional layers and to show an accuracy comparison between them.\n",
    "\n",
    "The architecture of a DAE has the following form:\n",
    "\n",
    "$$\n",
    "\\\n",
    "[Input] ‚Üí [Corruption] ‚Üí [Encoder] ‚Üí [Latent Code] ‚Üí [Decoder] ‚Üí [Reconstruction]\n",
    "\\\n",
    "$$\n",
    "\n",
    "$[Input]$ is self-explanatory. $[Corruption]$ is the stage where we inject artificial noise to the input through some noise process $C(\\hat{x} ‚à£ x)$ and obtain $\\hat{x}$, a corrupted version of the original input. In this work the corruption process is done via standard Gaussian noise, so:\n",
    "\n",
    "$$\n",
    "\\\n",
    "\\hat{x} = x + œµ, œµ ‚àº ùí©(0, œÉ^2)\n",
    "\\\n",
    "$$\n",
    "\n",
    "$[Encoder]$ $f()$ maps noisy input $\\hat{x}$ to latent representation $z$:\n",
    "\n",
    "$$\n",
    "\\\n",
    "z = f(\\hat{x}) ‚àà ‚Ñù·µà\n",
    "\\\n",
    "$$\n",
    "\n",
    "where $d < input\\_dim$. Learned $[Latent Code]$ $z$ should be robust to partial destruction of the input.\n",
    "$[Decoder]$ $g()$ reconstructs clean input from $z$:\n",
    "\n",
    "$$\n",
    "\\\n",
    "x' = g(z) ‚âà x\n",
    "\\\n",
    "$$\n",
    "\n",
    "where $x'$ is our $[Reconstruction]$.\n",
    "\n",
    "The \"magic\" of DAEs lies in their bottleneck architecture. As data passes through the encoder, the network gradually compresses the input while stripping away noise and preserving only the most essential features. This forced dimensionality reduction creates a distilled latent representation containing only the core patterns needed for reconstruction. The decoder then reverses this process, carefully rebuilding the clean input from these compressed features layer by layer. By training on noisy-clean pairs, the network learns to discard random corruptions during compression while maintaining the structural integrity needed for accurate reconstruction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e77352b",
   "metadata": {},
   "source": [
    "dare una piccola mathematical view\n",
    "\n",
    "fare review del codice, pulirlo sitemarlo\n",
    "\n",
    "introdurre l'implementazione e spiegarne le scelte\n",
    "\n",
    "codice fino al training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mid3_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
